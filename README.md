# FrameDiffusion

A frame2frame, video2video Video Editor based on the stable-diffusion

## Survey

* [StableDiffusion img2img API example](https://gist.github.com/w-e-w/0f37c04c18e14e4ee1482df5c4eb9f53)

### T2V ~ text2video

* [nateraw / stable-diffusion-videos](https://github.com/nateraw/stable-diffusion-videos) : 4.1k
* [Picsart-AI-Research / Text2Video-Zero](https://github.com/Picsart-AI-Research/Text2Video-Zero) : 3.7k
* [lucidrains / video-diffusion-pytorch](https://github.com/lucidrains/video-diffusion-pytorch) : 1.1k

### V&I2V ~ video2video & image2video

* Make-A-Video: [Site:MetaAI](https://makeavideo.studio/)
  and [SearchPaper: Cornell University / arXiv:2209.14792](https://arxiv.org/abs/2209.14792)

### V2V ~ vide2video

* [showlab / Tune-A-Video](https://github.com/showlab/Tune-A-Video) : 4k
* [omerbt / TokenFlow](https://github.com/omerbt/TokenFlow) : 1.4k
* [rese1f / StableVideo](https://github.com/rese1f/StableVideo) : 1.3k
* [ChenyangQiQi / FateZero](https://github.com/ChenyangQiQi/FateZero) : 1k

### I2V ~ image2video

* [AILab-CVC / VideoCrafter](https://github.com/AILab-CVC/VideoCrafter) : 3.8k
* [ali-vilab / i2vgen-xl](https://github.com/ali-vilab/i2vgen-xl) : 2.3k

### others survey

* [showlab / Awesome-Video-Diffusion](https://github.com/showlab/Awesome-Video-Diffusion) : 2k
* [ChenHsing / Awesome-Video-Diffusion-Models](https://github.com/ChenHsing/Awesome-Video-Diffusion-Models) : 990

## Preliminary investigation

#### Core

* [leandromoreira/digital_video_introduction](https://github.com/leandromoreira/digital_video_introduction):
  There are more theoretical studies on video

#### GUI

* [gradio docs](https://www.gradio.app/docs)
* TkInter and [CustomTKinter](https://github.com/TomSchimansky/CustomTkinter)
  and [CustomTkinter docs](https://customtkinter.tomschimansky.com/documentation/)
* [ParthJadhav / Tkinter-Designer](https://github.com/ParthJadhav/Tkinter-Designer)
* [PyQtGraph](https://www.pyqtgraph.org/)
